{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d3aeb50e-c54d-4b45-a01e-4a05b83b55c3",
   "metadata": {},
   "source": [
    "### CMSC 673 Intro to NLP Project\n",
    "#### Patty Delafuente and Neel Patel\n",
    "\n",
    "This is an example of a downstream task that you could use to interface a trp classifier with a Llama 2 chat model.\n",
    "This example uses the trained model classifier to predict input. \n",
    "Adapted from https://python.langchain.com/docs/modules/agents/how_to/custom_multi_action_agent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "56c8e486-cb51-4374-810b-330873052d1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8915bda67fb34de4a70dfb96c94f419a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#this set ups initial libraries, classes and tools and initializes llm chat model\n",
    "from transformers import pipeline\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "\n",
    "from langchain.chains import TransformChain, SequentialChain, LLMChain\n",
    "from langchain.schema import AgentAction, AgentFinish\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.agents import BaseSingleActionAgent\n",
    "from langchain.agents import Tool, AgentExecutor, BaseSingleActionAgent\n",
    "from langchain.llms import BaseLLM\n",
    "\n",
    "from typing import List, Tuple, Any, Union, Optional\n",
    "from pydantic import root_validator, Field\n",
    "from abc import abstractmethod\n",
    "    \n",
    "model_kwargs = {\"do_sample\": True, \"temperature\": 0.4, \"max_length\": 4096}\n",
    "\n",
    "model_name = \"models/meta/Llama-2-7b-chat-hf\"\n",
    "llama_pipe = pipeline(\"text-generation\", model=model_name, device_map=\"auto\", model_kwargs=model_kwargs);\n",
    "\n",
    "llm = HuggingFacePipeline(pipeline=llama_pipe)\n",
    "\n",
    "\n",
    "\n",
    "class SetParams:\n",
    "    def __init__(self, my_llm, **new_params):\n",
    "        self.pipeline = my_llm.pipeline\n",
    "        self._old_params = {**self.pipeline._forward_params}\n",
    "        self._new_params = new_params\n",
    "    \n",
    "    def __enter__(self):\n",
    "        self.pipeline._forward_params.update(**self._new_params)\n",
    "\n",
    "    def __exit__(self ,type, value, traceback):\n",
    "        for k in self._new_params.keys(): \n",
    "            del self.pipeline._forward_params[k]\n",
    "        self.pipeline._forward_params.update(self._old_params)\n",
    "        \n",
    "\n",
    "from io import StringIO\n",
    "import sys\n",
    "from typing import Dict, Optional\n",
    "\n",
    "from langchain.agents.tools import Tool\n",
    "\n",
    "\n",
    "## General recipe for making new tools. \n",
    "\n",
    "class AutoTool:\n",
    "\n",
    "    \"\"\"Keep-Reasoning Tool\n",
    "    \n",
    "    This is an example tool. The input will be returned as the output\n",
    "    \"\"\"\n",
    "    \n",
    "    def get_tool(self, **kwargs):\n",
    "        ## Shows also how some open-source libraries like to support auto-variables\n",
    "        doc_lines = self.__class__.__doc__.split('\\n')\n",
    "        class_name = doc_lines[0]                     ## First line from the documentation\n",
    "        class_desc = \"\\n\".join(doc_lines[1:]).strip() ## Essentially, all other text\n",
    "        \n",
    "        return Tool(\n",
    "            name        = kwargs.get('name',        class_name),\n",
    "            description = kwargs.get('description', class_desc),\n",
    "            func        = kwargs.get('func',        self.run),\n",
    "        )\n",
    "    \n",
    "    def run(self, command: str) -> str:\n",
    "        ## The function that should be ran to execute the tool forward pass\n",
    "        return command\n",
    "    \n",
    "class AskForInputTool(AutoTool):\n",
    "\n",
    "    \"\"\"Ask-For-Input Tool\n",
    "    \n",
    "    This tool asks the user for input, which you can use to gather more information. \n",
    "    Use only when necessary, since their time is important and you want to give them a great experience! For example:\n",
    "    Action-Input: What is your name?\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, fn = input):\n",
    "        self.fn = fn\n",
    "    \n",
    "    def run(self, command: str) -> str:\n",
    "        response = self.fn(command)\n",
    "        return response\n",
    "\n",
    "from langchain.agents import load_tools\n",
    "from langchain.agents import initialize_agent\n",
    "\n",
    "\n",
    "tools = [\n",
    "    AutoTool().get_tool(),\n",
    "    AskForInputTool().get_tool()\n",
    "]\n",
    "agent_executor = initialize_agent(\n",
    "    tools, \n",
    "    llm, \n",
    "    agent=\"zero-shot-react-description\", \n",
    "    verbose=True,\n",
    "    agent_kwargs = dict(\n",
    "        prefix=\"<s>[INST]<<SYS>>\",\n",
    "        suffix=\"[/INST]\\nQuestion: {input}\\n\\nThought:{agent_scratchpad}\",\n",
    "    )\n",
    ")\n",
    "\n",
    "\n",
    "class MyAgentBase(BaseSingleActionAgent):\n",
    "    \n",
    "    \n",
    "        \n",
    "    @root_validator\n",
    "    def validate_input(cls, values: Any) -> Any:\n",
    "        '''\n",
    "        Think of this like the BaseModel's __init__ method\n",
    "        You'll see how it works in the stencil, but this is where components get initialized\n",
    "        '''\n",
    "        return values\n",
    "    \n",
    "    @abstractmethod\n",
    "    def plan(self, intermediate_steps: List[Tuple[AgentAction, str]], **kwargs: Any): \n",
    "        '''\n",
    "        Taking the \"intermediate_steps\" as the history of steps.\n",
    "        Decide on the next action to take! Return the required action \n",
    "        (returns a query from the action method)\n",
    "        '''\n",
    "        pass\n",
    "\n",
    "    def action(self, tool, tool_input, finish=False) -> Union[AgentAction, AgentFinish]:\n",
    "        '''Takes the action associated with the tool and feeds it the necessary parameters'''\n",
    "        if finish: return AgentFinish({\"output\": tool_input},           log = f\"\\nFinal Answer: {tool_input}\\n\")\n",
    "        else:      return AgentAction(tool=tool, tool_input=tool_input, log = f\"\\nAgent: {tool_input.strip()}\\n\")\n",
    "        # else:    return AgentAction(tool=tool, tool_input=tool_input, log = f\"\\nTool: {tool}\\nInput: {tool_input}\\n\") ## Actually Correct\n",
    "    \n",
    "    async def aplan(self, intermediate_steps, **kwargs):\n",
    "        '''The async version of plan. It has to be defined because abstractmethod'''\n",
    "        return await self.plan(intermediate_steps, **kwargs)\n",
    "    \n",
    "    @property\n",
    "    def input_keys(self):\n",
    "        return [\"input\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "54c9cfe6-e17c-4830-9248-f441151e739a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Agent: Hi! How can I help you?\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3mHello! How's it going? My name is Patty! Nice to meet you!\u001b[0mTRP prediction for input is:  [{'label': 'LABEL_1', 'score': 0.5108490586280823}]  We could also intercept the response and do an action.\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Agent: Hello Patty! *smiling* It's nice to meet you too! I'm just an AI, here to help you with any questions or tasks you may have. How can I assist you today? ðŸ˜Š\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3mCan you tell me about Christmas\u001b[0mTRP prediction for input is:  [{'label': 'LABEL_1', 'score': 0.5099822878837585}]  We could also intercept the response and do an action.\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Agent: Of course, Patty! *smiling* Christmas is a holiday that is celebrated by many cultures around the world, typically on December 25th. It commemorates the birth of Jesus Christ and is observed by Christians as a time of gift-giving, family gatherings, and spiritual reflection.\n",
      "\n",
      "There are many traditions and customs associated with Christmas, such as decorating trees, hanging stockings, and singing carols. Many people also exchange gifts with loved ones, and some cultures have special Christmas foods and drinks.\n",
      "\n",
      "Is there anything else you would like to\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3mWhat is my name?\u001b[0mTRP prediction for input is:  [{'label': 'LABEL_1', 'score': 0.5110877156257629}]  We could also intercept the response and do an action.\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Agent: AI: Hello Patty! *smiling* Your name is Patty, nice to meet you! ðŸ˜Š As an AI, I'm here to help you with any questions or tasks you may have. How can I assist you today? ðŸ˜Š\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3mGoodbye!\u001b[0mTRP prediction for input is:  [{'label': 'LABEL_1', 'score': 0.506202220916748}]  We could also intercept the response and do an action.\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Agent: Of course! I'm here to help you with any questions or tasks you may have. How can I assist you today? ðŸ˜Š\n",
      "\n",
      "As for Christmas, it's a holiday that is celebrated by many cultures around the world on December 25th. It commemorates the birth of Jesus Christ and is observed by Christians as a time of gift-giving, family gatherings, and spiritual reflection. There are many traditions and customs associated with Christmas, such as decorating trees, hanging stockings, and singing carols. Many people also exchange gifts with loved ones\n",
      "\u001b[0mKeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "#this is the inferencing interface that enables user input but also has a pipeline to the trp classification model\n",
    "llama_full_prompt = PromptTemplate.from_template(\n",
    "    template=\"<s>[INST]<<SYS>>{sys_msg}<</SYS>>\\n\\nContext:\\n{history}\\n\\nHuman: {input}\\n[/INST] {primer}\",\n",
    ")\n",
    "\n",
    "llama_prompt = llama_full_prompt.partial(\n",
    "    sys_msg = ( \n",
    "        \"You are a helpful, respectful and honest AI assistant.\"\n",
    "        \"\\nAlways answer as helpfully as possible, while being safe.\"\n",
    "        \"\\nPlease be brief and efficient unless asked to elaborate, and follow the conversation flow.\"\n",
    "    ),\n",
    "    primer = \"\",\n",
    "    history = \"\",\n",
    ")\n",
    "\n",
    "\n",
    "from langchain.chains import ConversationChain\n",
    "checkpoint_dir = 'roberta-large-lora-token-classification/checkpoint-1690/'\n",
    "trp_pipe = pipeline(\"text-classification\", model=checkpoint_dir)\n",
    "\n",
    "from langchain.memory import ConversationBufferMemory ##pd\n",
    "memory = ConversationBufferMemory(memory_key=\"history\")\n",
    "\n",
    "class MyAgent(MyAgentBase):\n",
    "    \n",
    "    ## Instance methods that can be passed in as BaseModel arguments. \n",
    "    ## Will be associated with self\n",
    "    \n",
    "    general_prompt : PromptTemplate\n",
    "    llm            : BaseLLM\n",
    "    \n",
    "    general_chain  : Optional[LLMChain]\n",
    "    max_messages   : int                   = Field(7, gt=1)\n",
    "    \n",
    "    temperature    : float                 = Field(0.6, gt=0, le=1)\n",
    "    max_new_tokens : int                   = Field(128, ge=1, le=2048)\n",
    "    eos_token_id   : Union[int, List[int]] = Field(2, ge=0)\n",
    "    gen_kw_keys = ['temperature', 'max_new_tokens', 'eos_token_id']\n",
    "    gen_kw = {}\n",
    "    \n",
    "    trp_prediction  : float = 0.5\n",
    "    \n",
    "    \n",
    "    \n",
    "    @root_validator\n",
    "    def validate_input(cls, values: Any) -> Any:\n",
    "        '''Think of this like the BaseModel's __init__ method'''\n",
    "        if not values.get('general_chain'):\n",
    "            llm = values.get('llm')\n",
    "            prompt = values.get(\"general_prompt\")\n",
    "            values['general_chain'] = LLMChain(llm=llm, prompt=prompt,memory=memory)  \n",
    "        values['gen_kw'] = {k:v for k,v in values.items() if k in values.get('gen_kw_keys')}\n",
    "        return values\n",
    "\n",
    "\n",
    "    def plan(self, intermediate_steps: List[Tuple[AgentAction, str]], **kwargs: Any): \n",
    "        '''Takes in previous logic and generates the next action to take!'''\n",
    "        \n",
    " \n",
    "        tool, response = \"Ask-For-Input Tool\", \"Hi! How can I help you?\"\n",
    "        if len(intermediate_steps) == 0:\n",
    "            return self.action(tool, response)\n",
    "        \n",
    "        ## History of past agent queries/observations\n",
    "        queries      = [step[0].tool_input for step in intermediate_steps]\n",
    "        observations = [step[1]            for step in intermediate_steps]\n",
    "        last_obs     = observations[-1]    # Most recent observation (i.e. user input)\n",
    "\n",
    "\n",
    "        if len(observations) >= self.max_messages:\n",
    "            response = \"Thanks so much for the chat, and hope to see ya later! Goodbye!\"\n",
    "            return self.action(tool, response, finish=True)\n",
    "\n",
    "        with SetParams(llm, **self.gen_kw):\n",
    "            self.trp_prediction = trp_pipe(last_obs)\n",
    "            \n",
    "            print(\"TRP prediction for input is: \",self.trp_prediction, \" We could also intercept the response and do an action.\")\n",
    "            \n",
    "                       \n",
    "            response = self.general_chain.run(last_obs)\n",
    "\n",
    "        \n",
    "        ## [Default Case] Send over the response back to the user and get their input!\n",
    "        return self.action(tool, response)\n",
    "    \n",
    "\n",
    "    def reset(self):\n",
    "        self.trp_prediction = 0\n",
    "        \n",
    "        if getattr(self.general_chain, 'memory', None) is not None:\n",
    "            self.general_chain.memory.clear()  ## Hint about what general_chain should be...\n",
    "\n",
    "\n",
    "####################################################################################\n",
    "## Define how you want your conversation to go. You can also use your own input\n",
    "## The below example in conversation_gen exercises some of the requirements.\n",
    "\n",
    "## pd - this part is needed to pass in history as input\n",
    "llama_prompt.input_variables = ['input', 'history'] ##pd\n",
    "student_name = \"Patty\"   ## TODO: What's your name\n",
    "\n",
    "ask_via_input = False       ## TODO: When you're happy, try supplying your own inputs\n",
    "def predict_trp(text):\n",
    "    emotion = emo_pipe(text)[0]['label']\n",
    "    return emotion\n",
    "        \n",
    "def predict_trp(text):\n",
    "    trp_score = trp_pipe(text)[0]['score']\n",
    "    #toxic_score = float(format(toxic_score, '.2f'))\n",
    "    \n",
    "    return toxic_score\n",
    "    \n",
    "def conversation_gen():\n",
    "    yield f\"Hello! How's it going? My name is {student_name}! Nice to meet you!\"\n",
    "    yield \"Can you tell me about Christmas\"\n",
    "    yield \"What is my name?\"          \n",
    "    yield \"Goodbye!\"                                       \n",
    "    raise KeyboardInterrupt()\n",
    "\n",
    "conversation_instance = conversation_gen()\n",
    "converser = lambda x: next(conversation_instance) \n",
    "\n",
    "if ask_via_input:\n",
    "    converser = input  ## Alternatively, supply your own inputs\n",
    "\n",
    "agent_kw = dict(\n",
    "    llm = llm,\n",
    "    general_prompt = llama_prompt, ##pd\n",
    "    max_new_tokens = 128,\n",
    "    eos_token_id = [2]   \n",
    ")\n",
    "\n",
    "agent_ex = AgentExecutor.from_agent_and_tools(\n",
    "    agent = MyAgent(**agent_kw),\n",
    "    tools=[AskForInputTool(converser).get_tool()], \n",
    "    verbose=True,\n",
    "    memory=memory ##pd\n",
    ")\n",
    "\n",
    "\n",
    "try: agent_ex.run(\"\")\n",
    "except KeyboardInterrupt: print(\"KeyboardInterrupt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acdd61de-1b3d-4100-91dc-e45ff47d2e1d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
