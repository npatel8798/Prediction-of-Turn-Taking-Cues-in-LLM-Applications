{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3619f2e0-45b2-4252-8fc6-aea4c1d75bc2",
   "metadata": {},
   "source": [
    "### CMSC 673 Intro to NLP Final Project \n",
    "#### Patty Delafuente and Neel Patel\n",
    "Work split:\n",
    "Patty - approach and model training, proposal and document editing, inferencing and next step  \n",
    "Neel - First paper draft, approach review and model evaluation, review  \n",
    "Date: Dec 20, 2023"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7b5cfe82-f4b8-449f-929f-eefe6e1468a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# adapted work from https://github.com/mehdiir/Roberta-Llama-Mistral/blob/main/Lora-for-sequence-classification-with-Roberta-Llama-Mistral.md\n",
    "import torch\n",
    "MAX_LEN = 512 \n",
    "roberta_checkpoint = \"roberta-large\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "20a9ca46-55c8-475c-ab36-10502e4da577",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d41ea0ae-f978-4856-a324-15c24ad4ffe8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 6742 entries, 0 to 6741\n",
      "Data columns (total 3 columns):\n",
      " #   Column    Non-Null Count  Dtype \n",
      "---  ------    --------------  ----- \n",
      " 0   sentence  6742 non-null   object\n",
      " 1   target    6742 non-null   int64 \n",
      " 2   idx       6742 non-null   int64 \n",
      "dtypes: int64(2), object(1)\n",
      "memory usage: 158.1+ KB\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1062 entries, 0 to 1061\n",
      "Data columns (total 3 columns):\n",
      " #   Column    Non-Null Count  Dtype \n",
      "---  ------    --------------  ----- \n",
      " 0   sentence  1062 non-null   object\n",
      " 1   target    1062 non-null   int64 \n",
      " 2   idx       1062 non-null   int64 \n",
      "dtypes: int64(2), object(1)\n",
      "memory usage: 25.0+ KB\n"
     ]
    }
   ],
   "source": [
    "#dataprep\n",
    "import pandas as pd\n",
    "import os\n",
    "#use \n",
    "\n",
    "#train_df=pd.read_csv('df_train.csv')\n",
    "train_df = pd.read_csv('training_data.csv')\n",
    "#val_df=pd.read_csv('df_val.csv')\n",
    "test_df=pd.read_csv('df_test.csv')\n",
    "# dummy target column for merge test and train into one huggingface data\n",
    "#test\n",
    "test_df['target'] = 0 \n",
    "train_df.info()\n",
    "#val_df.info()\n",
    "test_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "44ac83fb-0f8c-4a9a-8d34-17cd551fbef1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>target</th>\n",
       "      <th>idx</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One more pseudo generalization and I'm giving up.</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>One more pseudo generalization or I'm giving up.</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The more we study verbs, the crazier they get.</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Day by day the facts are getting murkier.</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I'll fix you a drink.</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            sentence  target  idx\n",
       "0  One more pseudo generalization and I'm giving up.       0    1\n",
       "1   One more pseudo generalization or I'm giving up.       0    2\n",
       "2     The more we study verbs, the crazier they get.       0    3\n",
       "3          Day by day the facts are getting murkier.       0    4\n",
       "4                              I'll fix you a drink.       0    5"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()\n",
    "#target is the trp label, I just labeled everything with a question mark as 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8e125783-a1ca-4a45-b978-9bc1ae837751",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d7c18c9079449ea84b89314ef47ee29",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/5393 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29fb26c459224e7895d08e14dcef4ab7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/1349 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ab219ec1c6a4ab49bebe74290fae34b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/1062 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Patty\n",
    "# adapted work from https://github.com/mehdiir/Roberta-Llama-Mistral/blob/main/Lora-for-sequence-classification-with-Roberta-Llama-Mistral.md\n",
    "\n",
    "#unable to use validation dataset I created - issue with huggingface (or my brain) format. Instead, the training dataset will be split\n",
    "from datasets import Dataset\n",
    "#Convert the training dataframe to HuggingFace dataset\n",
    "dataset = Dataset.from_pandas(train_df)\n",
    "#Split the dataset into training and validation datasets\n",
    "data = dataset.train_test_split(train_size=0.8, seed=42)\n",
    "#Rename the default \"test\" split to \"validation\"\n",
    "data['val'] = data.pop(\"test\")\n",
    "#Convert the test dataframe to HuggingFace dataset and add it into the first dataset\n",
    "data['test'] = Dataset.from_pandas(test_df)\n",
    "#Save\n",
    "data.save_to_disk(\"processed_hf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d15075df-67bc-4611-9415-deec89685117",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load roberta tokenizer\n",
    "from transformers import AutoTokenizer\n",
    "roberta_tokenizer = AutoTokenizer.from_pretrained(roberta_checkpoint, add_prefix_space=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9413afc8-e203-4734-a9c7-208ef8e74a6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def roberta_preprocessing_function(examples):\n",
    "    return roberta_tokenizer(examples['sentence'], truncation=True, max_length=MAX_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e9825e6c-e241-4e3f-aff4-315f9f55a78c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [0, 20, 2335, 439, 5373, 4, 2], 'attention_mask': [1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roberta_preprocessing_function(data['train'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "57fa44bf-62de-47fa-a3e2-c72d4b64692b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd351261363f4c5ba66f364c9a9115f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5393 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20fd0d9b948c48e08181391f89b6fd4b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1349 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7de03ac982cd4b03b13b320f7561560c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1062 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Patty \n",
    "# adapted work from https://github.com/mehdiir/Roberta-Llama-Mistral/blob/main/Lora-for-sequence-classification-with-Roberta-Llama-Mistral.md\n",
    "\n",
    "#apply to entire dataset\n",
    "col_to_delete = ['idx', 'sentence']\n",
    "#Apply the preprocessing function\n",
    "roberta_tokenized_datasets = data.map(roberta_preprocessing_function, batched=False)\n",
    "#Remove the undesired columns\n",
    "roberta_tokenized_datasets = roberta_tokenized_datasets.remove_columns(col_to_delete)\n",
    "#Rename the target to label as for HugginFace standards\n",
    "roberta_tokenized_datasets = roberta_tokenized_datasets.rename_column(\"target\", \"label\")\n",
    "#Set to torch format\n",
    "roberta_tokenized_datasets.set_format(\"torch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9bc1142c-6cc6-47ef-ac1a-d7a88cea40e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'label': tensor(1), 'input_ids': tensor([    0,  3394,   222,    47,   972,    70,    77,    47,    58,    11,\n",
      "          211, 11228,   116,     2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n"
     ]
    }
   ],
   "source": [
    "print(roberta_tokenized_datasets['train'][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "19b6150a-09ab-4161-bb75-442f6b3896e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data collator for padding a batch of examples to the maximum length seen in the batch\n",
    "from transformers import DataCollatorWithPadding\n",
    "roberta_data_collator = DataCollatorWithPadding(tokenizer=roberta_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "57ce26b9-9d27-4ef2-8285-d8bb1c650352",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.out_proj.weight', 'classifier.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification \n",
    "# Load a pre-trained model with a sequence classification header \n",
    "roberta_model = AutoModelForSequenceClassification.from_pretrained(roberta_checkpoint, num_labels=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ba8c714b-c05a-4733-97f5-3a105a9925bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'label': tensor(1), 'input_ids': tensor([    0,  3394,   222,    47,   972,    70,    77,    47,    58,    11,\n",
      "          211, 11228,   116,     2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "{'label': tensor(0), 'input_ids': tensor([    0,    38,    33,    10,  2157,    14, 10832,    40,   311,    62,\n",
      "            4,     2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}\n",
      "{'label': tensor(0), 'input_ids': tensor([    0,  1585,  3148,  3268,   160,     5, 26711,     4,     2]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1])}\n"
     ]
    }
   ],
   "source": [
    "print(roberta_tokenized_datasets['train'][1])\n",
    "print(roberta_tokenized_datasets['val'][1])\n",
    "print(roberta_tokenized_datasets['test'][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2c99409c-67e6-4d6e-99ef-6238b65814dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Patty\n",
    "# adapted work from https://github.com/mehdiir/Roberta-Llama-Mistral/blob/main/Lora-for-sequence-classification-with-Roberta-Llama-Mistral.md\n",
    "\n",
    "#For generating the training batches, we also need to pad the rows of a given batch to the same maximum length, the maximum length found in this batch.\n",
    "#For that, we will use the DataCollatorWithPadding class:\n",
    "# Data collator for padding a batch of examples to the maximum length seen in the batch\n",
    "from transformers import DataCollatorWithPadding\n",
    "# roberta_data_collator = DataCollatorWithPadding(tokenizer=roberta_tokenizer)\n",
    "roberta_data_collator = DataCollatorWithPadding(\n",
    "    tokenizer=roberta_tokenizer,\n",
    "    padding=\"max_length\",  # or any other appropriate padding strategy\n",
    "    max_length=MAX_LEN,\n",
    "    return_tensors=\"pt\",  # Ensure the output is PyTorch tensors\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1e246eb3-aefb-4d69-af4e-1b4bb736e510",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.out_proj.weight', 'classifier.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "# Load a pre-trained model with a sequence classification header\n",
    "roberta_model = AutoModelForSequenceClassification.from_pretrained(roberta_checkpoint, num_labels=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4ca5f330-e391-47d9-b691-70b94474ce8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 1,248,258 || all params: 356,610,052 || trainable%: 0.35003444042009224\n"
     ]
    }
   ],
   "source": [
    "#Patty\n",
    "# adapted work from https://github.com/mehdiir/Roberta-Llama-Mistral/blob/main/Lora-for-sequence-classification-with-Roberta-Llama-Mistral.md\n",
    "\n",
    "#lora setup for Roberta classifier\n",
    "from peft import get_peft_model, LoraConfig, TaskType\n",
    "roberta_peft_config = LoraConfig(\n",
    "    task_type=TaskType.SEQ_CLS, r=2, lora_alpha=16, lora_dropout=0.1, bias=\"none\",\n",
    ")\n",
    "roberta_model = get_peft_model(roberta_model, roberta_peft_config)\n",
    "roberta_model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8fd74c63-ed1c-412c-9ee4-07aad045000f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Neel\n",
    "# adapted work from https://github.com/mehdiir/Roberta-Llama-Mistral/blob/main/Lora-for-sequence-classification-with-Roberta-Llama-Mistral.md\n",
    "\n",
    "#evaluation metrics\n",
    "#Define evaluation metrics\n",
    "import evaluate\n",
    "import numpy as np\n",
    "def compute_metrics(eval_pred):\n",
    "    # All metrics are already predefined in the HF `evaluate` package\n",
    "    precision_metric = evaluate.load(\"precision\")\n",
    "    recall_metric = evaluate.load(\"recall\")\n",
    "    f1_metric= evaluate.load(\"f1\")\n",
    "    accuracy_metric = evaluate.load(\"accuracy\")\n",
    "\n",
    "    logits, labels = eval_pred # eval_pred is the tuple of predictions and labels returned by the model\n",
    "    # print(logits)\n",
    "    # print(labels)\n",
    "    # print(logits, len(logits))\n",
    "    # print(labels, len(labels))\n",
    "    # predictions = np.argmax(logits, axis=-1)\n",
    "    # predictions = [predictions]\n",
    "    # labels = [labels]\n",
    "    predictions = logits\n",
    "    print('predictions:', predictions)\n",
    "    print('labels:', labels)\n",
    "    precision = precision_metric.compute(predictions=predictions, references=labels)[\"precision\"]\n",
    "    recall = recall_metric.compute(predictions=predictions, references=labels)[\"recall\"]\n",
    "    f1 = f1_metric.compute(predictions=predictions, references=labels)[\"f1\"]\n",
    "    accuracy = accuracy_metric.compute(predictions=predictions, references=labels)[\"accuracy\"]\n",
    "    # The trainer is expecting a dictionary where the keys are the metrics names and the values are the scores.\n",
    "    return {\"precision\": precision, \"recall\": recall, \"f1-score\": f1, 'accuracy': accuracy}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8b0d46fd-fc95-4e33-a89d-f9aaae59adac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PeftModelForSequenceClassification(\n",
       "  (base_model): LoraModel(\n",
       "    (model): RobertaForSequenceClassification(\n",
       "      (roberta): RobertaModel(\n",
       "        (embeddings): RobertaEmbeddings(\n",
       "          (word_embeddings): Embedding(50265, 1024, padding_idx=1)\n",
       "          (position_embeddings): Embedding(514, 1024, padding_idx=1)\n",
       "          (token_type_embeddings): Embedding(1, 1024)\n",
       "          (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (encoder): RobertaEncoder(\n",
       "          (layer): ModuleList(\n",
       "            (0-23): 24 x RobertaLayer(\n",
       "              (attention): RobertaAttention(\n",
       "                (self): RobertaSelfAttention(\n",
       "                  (query): Linear(\n",
       "                    in_features=1024, out_features=1024, bias=True\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=1024, out_features=2, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=2, out_features=1024, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                  )\n",
       "                  (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                  (value): Linear(\n",
       "                    in_features=1024, out_features=1024, bias=True\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=1024, out_features=2, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=2, out_features=1024, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                  )\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (output): RobertaSelfOutput(\n",
       "                  (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                  (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (intermediate): RobertaIntermediate(\n",
       "                (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "                (intermediate_act_fn): GELUActivation()\n",
       "              )\n",
       "              (output): RobertaOutput(\n",
       "                (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "                (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (classifier): ModulesToSaveWrapper(\n",
       "        (original_module): RobertaClassificationHead(\n",
       "          (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (out_proj): Linear(in_features=1024, out_features=2, bias=True)\n",
       "        )\n",
       "        (modules_to_save): ModuleDict(\n",
       "          (default): RobertaClassificationHead(\n",
       "            (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (out_proj): Linear(in_features=1024, out_features=2, bias=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Patty\n",
    "# adapted work from https://github.com/mehdiir/Roberta-Llama-Mistral/blob/main/Lora-for-sequence-classification-with-Roberta-Llama-Mistral.md\n",
    "\n",
    "#trainer setup Roberta\n",
    "\n",
    "roberta_model= torch.nn.DataParallel(roberta_model)\n",
    "roberta_model = roberta_model.module  # Access the underlying model from DataParallel\n",
    "roberta_model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7f2fdb22-16d1-4649-86ae-b030272293f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Patty\n",
    "# adapted work from https://github.com/mehdiir/Roberta-Llama-Mistral/blob/main/Lora-for-sequence-classification-with-Roberta-Llama-Mistral.md\n",
    "\n",
    "#set training arguments\n",
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "lr = 1e-4\n",
    "batch_size = 8\n",
    "num_epochs = 5\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"roberta-large-lora-token-classification\",\n",
    "    learning_rate=lr,\n",
    "    lr_scheduler_type= \"constant\",\n",
    "    warmup_ratio= 0.1,\n",
    "    max_grad_norm= 0.3,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs=num_epochs,\n",
    "    weight_decay=0.001,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    #report_to=\"wandb\",\n",
    "    fp16=False,\n",
    "    gradient_checkpointing=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3a4db96c-3175-4208-95a7-7d51d12adfe9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<torch.utils.data.dataloader.DataLoader object at 0x7f63aa15a340>\n",
      "dict_keys(['label', 'input_ids', 'attention_mask'])\n",
      "DataCollatorWithPadding(tokenizer=RobertaTokenizerFast(name_or_path='roberta-large', vocab_size=50265, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'sep_token': '</s>', 'pad_token': '<pad>', 'cls_token': '<s>', 'mask_token': '<mask>'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
      "\t0: AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
      "\t1: AddedToken(\"<pad>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
      "\t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
      "\t3: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
      "\t50264: AddedToken(\"<mask>\", rstrip=False, lstrip=True, single_word=False, normalized=False, special=True),\n",
      "}, padding='max_length', max_length=512, pad_to_multiple_of=None, return_tensors='pt')\n",
      "{'label': tensor([0]), 'input_ids': tensor([[   0,  610,   34,   10, 2490,    9, 3678,    4,    2]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n",
      "dict_keys(['label', 'input_ids', 'attention_mask'])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a RobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "/usr/local/lib/python3.8/dist-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1690' max='1690' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1690/1690 11:08, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.053212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.080900</td>\n",
       "      <td>0.043433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.034100</td>\n",
       "      <td>0.029606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.034100</td>\n",
       "      <td>0.027315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.026100</td>\n",
       "      <td>0.026402</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.8/dist-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.8/dist-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/usr/local/lib/python3.8/dist-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1690, training_loss=0.04441779943612906, metrics={'train_runtime': 667.7067, 'train_samples_per_second': 40.384, 'train_steps_per_second': 2.531, 'total_flos': 2.523293051099136e+16, 'train_loss': 0.04441779943612906, 'epoch': 5.0})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Patty\n",
    "# adapted work from https://github.com/mehdiir/Roberta-Llama-Mistral/blob/main/Lora-for-sequence-classification-with-Roberta-Llama-Mistral.md\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "roberta_trainer = Trainer(\n",
    "    model=roberta_model,\n",
    "    args=training_args,\n",
    "    train_dataset=roberta_tokenized_datasets['train'],\n",
    "    eval_dataset=roberta_tokenized_datasets['val'],\n",
    "    data_collator=roberta_data_collator,\n",
    "    )\n",
    "\n",
    "data_loader = DataLoader(roberta_tokenized_datasets['train'], batch_size=1, shuffle=True)\n",
    "print(data_loader)\n",
    "print(roberta_tokenized_datasets['train'][0].keys())\n",
    "print(roberta_data_collator)\n",
    "\n",
    "for batch in data_loader:\n",
    "  print(batch)\n",
    "  print(batch.keys())\n",
    "  break\n",
    "\n",
    "    # Start training\n",
    "roberta_trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "76ea4f4e-1832-45ab-8cb4-97fcc8de955f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.out_proj.weight', 'classifier.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e49662f974f647f88e1e5277096b5194",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/1349 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "143445b56d4849c6a2e0d8d71ee2db0d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/7.55k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f15abd0302df4bb794263e017476dd46",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/7.36k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4fd922cd0d704b31bb62cae8faa33f16",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/6.77k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f20d94a6c91b46a096e0ea84af071a76",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/4.20k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predictions: [0 0 0 ... 0 0 0]\n",
      "labels: [0 0 0 ... 0 0 0]\n",
      "Evaluation Metrics:\n",
      "precision: 0.9583333333333334\n",
      "recall: 0.989247311827957\n",
      "f1-score: 0.9735449735449735\n",
      "accuracy: 0.9962935507783544\n"
     ]
    }
   ],
   "source": [
    "##Neel\n",
    "from tqdm.auto import tqdm\n",
    "from transformers import RobertaTokenizer\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "checkpoint_dir = 'roberta-large-lora-token-classification/checkpoint-1690/'\n",
    "loaded_model = AutoModelForSequenceClassification.from_pretrained(checkpoint_dir)\n",
    "# loaded_tokenizer = AutoTokenizer.from_pretrained(model_dir)\n",
    "# loaded_tokenizer = RobertaTokenizer.from_pretrained(model_dir)\n",
    "\n",
    "# Data collator for padding a batch of examples to the maximum length\n",
    "# test_data_collator = DataCollatorWithPadding(\n",
    "#     tokenizer=loaded_tokenizer,\n",
    "#     padding=\"max_length\",\n",
    "#     max_length=MAX_LEN,\n",
    "#     return_tensors=\"pt\",\n",
    "# )\n",
    "\n",
    "loaded_model = loaded_model.to(device)\n",
    "test_dataset = roberta_tokenized_datasets['val']\n",
    "test_data_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "all_labels = []\n",
    "all_predictions = []\n",
    "\n",
    "for batch in tqdm(test_data_loader, desc=\"Evaluating\"):\n",
    "    input_ids = batch[\"input_ids\"].to(device)\n",
    "    attention_mask = batch[\"attention_mask\"].to(device)\n",
    "    labels = batch[\"label\"].to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = loaded_model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "\n",
    "    logits = outputs.logits\n",
    "    predictions = torch.argmax(logits, dim=-1)\n",
    "\n",
    "    all_labels.extend(labels.cpu().numpy())\n",
    "    all_predictions.extend(predictions.cpu().numpy())\n",
    "\n",
    "# Convert to numpy arrays\n",
    "all_labels = np.array(all_labels)\n",
    "all_predictions = np.array(all_predictions)\n",
    "\n",
    "# Compute evaluation metrics using the provided compute_metrics function\n",
    "evaluation_metrics = compute_metrics((all_predictions, all_labels))\n",
    "\n",
    "# Display evaluation metrics\n",
    "print(\"Evaluation Metrics:\")\n",
    "for metric_name, metric_value in evaluation_metrics.items():\n",
    "    print(f\"{metric_name}: {metric_value}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "99755e42-32bc-46a7-9b26-6f8dcea92a43",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.out_proj.weight', 'classifier.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The chance we should switch speakers after this sentence is  0.6000781059265137\n"
     ]
    }
   ],
   "source": [
    "##Patty\n",
    "from transformers import pipeline\n",
    "#from langchain.llms import HuggingFacePipeline\n",
    "trp_pipe = pipeline(\"text-classification\", model=checkpoint_dir)\n",
    "text = \"What are you doing for the holidays?\"\n",
    "trp_score = trp_pipe(text)[0]['score']\n",
    "print(\"The chance we should switch speakers after this sentence is \",trp_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9bb11294-de4a-4015-93da-14d36606d657",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The chance we should switch speakers after this sentence is  0.5964725017547607\n"
     ]
    }
   ],
   "source": [
    "text = \"Do you want to watch a movie?\"\n",
    "trp_score = trp_pipe(text)[0]['score']\n",
    "print(\"The chance we should switch speakers after this sentence is \",trp_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e688ff9-12ad-43ba-9b3c-6e4e5442a2d0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
